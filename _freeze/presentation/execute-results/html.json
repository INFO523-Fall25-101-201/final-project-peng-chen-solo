{
  "hash": "f401e01d23a3bf45111d95ca82ff11d1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Build ML model on Synthetic Patient Data to Predict OSA Risk\"\nsubtitle: \"INFO 523 - Summer 2025 - Final Project\"\nauthor: \"Peng Chen\"\ntitle-slide-attributes:\n  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg\n  data-background-size: stretch\n  data-background-opacity: \"0.7\"\n  data-slide-number: none\nformat:\n  revealjs:\n    theme:  ['data/customtheming.scss']\n  \neditor: visual\njupyter: python3\nexecute:\n  echo: false\n---\n\n## Python modules used in this project\n\n::: panel-tabset\n## Models\n\n\n\n``` python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Preprocessing \nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\n\n# Model Selection and Evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    f1_score,\n    balanced_accuracy_score,\n    recall_score,\n    confusion_matrix\n)\n\nfrom sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector\n\n# Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nimport warnings\n```\n\n## Settings\n\n``` python\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n## Project outline\n\n1.  Generate synthetic data\n\n2.  Train an optimal ML model using synthetic data\n\n3.  Train an optima ML model using real data\n\n4.  Cross validation (real model & synthetic test data; synthetic model & real test data)\n\n5.  Result interpretaion\n\n## 1. Generate synthetic data\n\n::: panel-tabset\n## Load data\n\n::: {#7c30e654 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 221269 entries, 0 to 221268\nData columns (total 70 columns):\n #   Column       Non-Null Count   Dtype  \n---  ------       --------------   -----  \n 0   f_eid        221269 non-null  int64  \n 1   f_20016_0_0  70974 non-null   float64\n 2   f_20016_1_0  9711 non-null    float64\n 3   f_4282_0_0   23876 non-null   float64\n 4   f_4282_1_0   0 non-null       float64\n 5   f_20023_0_0  220118 non-null  float64\n 6   f_20023_1_0  9755 non-null    float64\n 7   IID          221269 non-null  int64  \n 8   PC1          221269 non-null  float64\n 9   PC2          221269 non-null  float64\n 10  Male         221269 non-null  int64  \n 11  BMI          221269 non-null  float64\n 12  Age          221269 non-null  float64\n 13  Date         2304 non-null    object \n 14  OSA          221269 non-null  int64  \n 15  APOE_e4      221269 non-null  int64  \n 16  snoring      221138 non-null  float64\n 17  centre       221269 non-null  object \n 18  f_25887_2_0  19426 non-null   float64\n 19  f_25887_3_0  762 non-null     float64\n 20  f_25886_2_0  19426 non-null   float64\n 21  f_25886_3_0  762 non-null     float64\n 22  f_25003_2_0  19429 non-null   float64\n 23  f_25003_3_0  762 non-null     float64\n 24  f_25004_2_0  19429 non-null   float64\n 25  f_25004_3_0  762 non-null     float64\n 26  f_25001_2_0  19429 non-null   float64\n 27  f_25001_3_0  762 non-null     float64\n 28  f_25002_2_0  19429 non-null   float64\n 29  f_25002_3_0  762 non-null     float64\n 30  f_25005_2_0  19429 non-null   float64\n 31  f_25005_3_0  762 non-null     float64\n 32  f_25006_2_0  19429 non-null   float64\n 33  f_25006_3_0  762 non-null     float64\n 34  f_25007_2_0  19429 non-null   float64\n 35  f_25007_3_0  762 non-null     float64\n 36  f_25008_2_0  19429 non-null   float64\n 37  f_25008_3_0  762 non-null     float64\n 38  f_25009_2_0  19429 non-null   float64\n 39  f_25009_3_0  762 non-null     float64\n 40  f_25010_2_0  19429 non-null   float64\n 41  f_25010_3_0  762 non-null     float64\n 42  f_25012_2_0  19423 non-null   float64\n 43  f_25012_3_0  762 non-null     float64\n 44  f_25011_2_0  19423 non-null   float64\n 45  f_25011_3_0  762 non-null     float64\n 46  f_25016_2_0  19423 non-null   float64\n 47  f_25016_3_0  762 non-null     float64\n 48  f_25015_2_0  19423 non-null   float64\n 49  f_25015_3_0  762 non-null     float64\n 50  f_25018_2_0  19423 non-null   float64\n 51  f_25018_3_0  762 non-null     float64\n 52  f_25017_2_0  19423 non-null   float64\n 53  f_25017_3_0  762 non-null     float64\n 54  f_25020_2_0  19423 non-null   float64\n 55  f_25020_3_0  762 non-null     float64\n 56  f_25019_2_0  19423 non-null   float64\n 57  f_25019_3_0  762 non-null     float64\n 58  f_25014_2_0  19423 non-null   float64\n 59  f_25014_3_0  762 non-null     float64\n 60  f_25013_2_0  19423 non-null   float64\n 61  f_25013_3_0  762 non-null     float64\n 62  f_25022_2_0  19423 non-null   float64\n 63  f_25022_3_0  762 non-null     float64\n 64  f_25021_2_0  19423 non-null   float64\n 65  f_25021_3_0  762 non-null     float64\n 66  f_25024_2_0  19423 non-null   float64\n 67  f_25024_3_0  762 non-null     float64\n 68  f_25023_2_0  19423 non-null   float64\n 69  f_25023_3_0  762 non-null     float64\ndtypes: float64(63), int64(5), object(2)\nmemory usage: 118.2+ MB\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nOSA\n0    218965\n1      2304\nName: count, dtype: int64\n```\n:::\n:::\n\n\n## Drop Missing \n\n::: {#2d35d3ae .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 19416 entries, 2 to 221264\nData columns (total 35 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PC1          19416 non-null  float64\n 1   PC2          19416 non-null  float64\n 2   Male         19416 non-null  int64  \n 3   BMI          19416 non-null  float64\n 4   Age          19416 non-null  float64\n 5   OSA          19416 non-null  int64  \n 6   APOE_e4      19416 non-null  int64  \n 7   snoring      19416 non-null  float64\n 8   centre       19416 non-null  object \n 9   f_25887_2_0  19416 non-null  float64\n 10  f_25886_2_0  19416 non-null  float64\n 11  f_25003_2_0  19416 non-null  float64\n 12  f_25004_2_0  19416 non-null  float64\n 13  f_25001_2_0  19416 non-null  float64\n 14  f_25002_2_0  19416 non-null  float64\n 15  f_25005_2_0  19416 non-null  float64\n 16  f_25006_2_0  19416 non-null  float64\n 17  f_25007_2_0  19416 non-null  float64\n 18  f_25008_2_0  19416 non-null  float64\n 19  f_25009_2_0  19416 non-null  float64\n 20  f_25010_2_0  19416 non-null  float64\n 21  f_25012_2_0  19416 non-null  float64\n 22  f_25011_2_0  19416 non-null  float64\n 23  f_25016_2_0  19416 non-null  float64\n 24  f_25015_2_0  19416 non-null  float64\n 25  f_25018_2_0  19416 non-null  float64\n 26  f_25017_2_0  19416 non-null  float64\n 27  f_25020_2_0  19416 non-null  float64\n 28  f_25019_2_0  19416 non-null  float64\n 29  f_25014_2_0  19416 non-null  float64\n 30  f_25013_2_0  19416 non-null  float64\n 31  f_25022_2_0  19416 non-null  float64\n 32  f_25021_2_0  19416 non-null  float64\n 33  f_25024_2_0  19416 non-null  float64\n 34  f_25023_2_0  19416 non-null  float64\ndtypes: float64(31), int64(3), object(1)\nmemory usage: 5.3+ MB\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nOSA\n0    19280\n1      136\nName: count, dtype: int64\n```\n:::\n:::\n\n\n:::\n\n## 1. Generate synthetic data\n\n### Train GAN\n\n::: panel-tabset\n## Note\n\n-   I employed SDV CTGAN synthesizer\n\n-   Key to success:\n\n    -   Conditional on OSA, to balance patients and control\n\n    -   Tune epochs and learning rate so that losses converge\n\n```         \nTraining SDV CTGAN synthesizer (conditional on OSA)... \n```\n\n```         \nGen. (-1.18) | Discrim. (-0.44): 100%|██████████| 1000/1000 [31:08<00:00,  1.87s/it]\n```\n\n## Loss function\n\n![](results/ukb_synthesizer_osa_loss.png){fig-align=\"center\"}\n:::\n\n## 1. Generate synthetic data\n\n### Data synthesis\n\n``` python\nn_samples = 40000 # total number of records to generate\nprint(f\"Generating {n_samples} synthetic samples balanced on osa_diag...\")\nsynth = generate_balanced_samples(\n    synthesizer=synthesizer,\n    n_samples=n_samples,\n    label_col=\"OSA\",\n)\n\nprint(\"Synthetic osa_diag value counts:\")\nprint(synth[\"OSA\"].value_counts(dropna=False))\n```\n\n```         \nGenerating 40000 synthetic samples balanced on osa_diag... \n```\n\n```         \nSampling conditions: 100%|██████████| 40000/40000 [00:16<00:00, 2455.99it/s]\n```\n\n```         \nSynthetic osa_diag value counts: \nOSA \n0    20000 \n1    20000 \nName: count, dtype: int64\n```\n\n## 1. Generate synthetic data\n\n::: panel-tabset\n## Struture evaluation\n\n``` python\nfrom sdv.evaluation.single_table import run_diagnostic\nmetadata = SingleTableMetadata()\nmetadata.detect_from_dataframe(dat)\ndiagnostic = run_diagnostic(\n    real_data=dat,\n    synthetic_data=synth,\n    metadata=metadata\n)\n```\n\n```         \nGenerating report ...  \n(1/2) Evaluating Data Validity: |██████████| 35/35 [00:00<00:00, 663.53it/s]| \nData Validity Score: 100.0%  \n(2/2) Evaluating Data Structure: |██████████| 1/1 [00:00<00:00, 391.88it/s]| \nData Structure Score: 100.0%  \nOverall Score (Average): 100.0%\n```\n\n## Similarity evaluation\n\n``` python\nfrom sdv.evaluation.single_table import evaluate_quality\nquality_report = evaluate_quality(\n    dat,\n    synth,\n    metadata\n)\n```\n\n```         \nGenerating report ...  \n(1/2) Evaluating Column Shapes: |██████████| 35/35 [00:00<00:00, 66.32it/s]| \nColumn Shapes Score: 91.65%  \n(2/2) Evaluating Column Pair Trends: |██████████| 595/595 [00:04<00:00, 121.03it/s]| \nColumn Pair Trends Score: 88.56%  \nOverall Score (Average): 90.1%\n```\n:::\n\n## 2. Train an optimal ML model\n\n::: panel-tabset\n## Preprocessing pipeline\n\n``` python\nnumeric_features = synth_X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncategorical_features = synth_X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop='first', sparse_output=False))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ]\n)\nX_transformed = preprocess.fit_transform(synth_X)\nprint(X_transformed.shape)\n```\n\n## PCA\n\n![](results/pca.png){fig-align=\"center\"}\n:::\n\n## 2. Train an optimal ML model\n\n::: panel-tabset\n## Feature selection\n\nI employed LASSO regression to select relevant features.\n\n``` python\n# 1) L1-logistic as selector\nl1_selector_est = LogisticRegression(\n    penalty=\"l1\",\n    solver=\"saga\",\n    class_weight=\"balanced\",\n    max_iter=1000\n)\n\nselector = SelectFromModel(\n    estimator=l1_selector_est,\n    max_features=X_transformed.shape[1],\n    threshold=\"median\"\n)\n\n# 2) Pipeline: preprocess -> L1 selector\nfs_pipe = Pipeline(steps=[\n    (\"preprocess\", preprocess),  \n    (\"select\", selector)\n])\n\n# Fit on TRAIN ONLY\nfs_pipe.fit(X_train, y_train)\n\n# 3) Get transformed feature names and support mask\npreprocess_fitted = fs_pipe.named_steps[\"preprocess\"]\nselector_fitted   = fs_pipe.named_steps[\"select\"]\n\nall_feature_names = preprocess_fitted.get_feature_names_out()\nsupport_mask      = selector_fitted.get_support()\n\nselected_feature_names = all_feature_names[support_mask]\n```\n\n## Selected features\n\n``` python\nprint(\"Number of selected features:\", len(selected_feature_names))\nfor f in selected_feature_names:\n    print(f)\n```\n\n```         \nNumber of selected features: 16 \nnum__PC1 num__BMI num__f_25886_2_0 num__f_25003_2_0 num__f_25001_2_0 num__f_25006_2_0 num__f_25007_2_0 num__f_25010_2_0 num__f_25015_2_0 num__f_25018_2_0 num__f_25019_2_0 num__f_25014_2_0 num__f_25013_2_0 num__f_25022_2_0 num__f_25021_2_0 cat__Male_1\n```\n:::\n\n## 2. Train an optimal ML model\n\n### Model selection\n\n-   Multiple models and a wide range of parameters.\n\n-   The optimal model: XGBoost model with learning_rate = 0.05, n_estimators=800 and max_depth=3. It has ROC_AUC=0.847.\n\n-   I tested the performance on synthetic test data:\n\n```         \n=== Final ML model on TEST set === \nROC-AUC:  0.849 \nAccuracy: 0.769 \nF1:       0.769 \nPrecision:0.770 \nRecall:   0.767 \nConfusion matrix:  \n[[3082  918]  \n[ 930 3070]]\n```\n\n## 3. Train an optima ML model using real data\n\nThe optimal model built on real data: XGBoost model, learning_rate=0.01, n_estimators=200, max_depth=3. ROC_AUC=0.856.\n\nPerformance on real test data:\n\n```         \n=== Final ML model on TEST set === \nROC-AUC:  0.856 \nAccuracy: 0.993 \nF1:       0.000 \nPrecision:0.000 \nRecall:   0.000 \nConfusion matrix:  \n[[3856    0]  \n[  29    0]]\n```\n\n## 4. Real x Synthetic cross validation\n\n::: panel-tabset\n## Note\n\n-   I further tested for two use cases of synthetic patient records\n\n    -   Case 1: use the model built on **synthetic data** to predict OSA risk in **real data**. This resolves the clinical problem where limited patient data is available to build a model to predict disease risk.\n\n    -   Case 2: use the model build on **real data** to predict OSA risk in **synthetic data**. This may be used to reduce the cost of clinical trails, where a large amount of participants are needed to access the efficacy and safety of a therapy.\n\n-   I evaluated the performance in these two cases.\n\n## Synthetic-\\>real\n\nUse model built on synthetic data to predict real data:\n\n```         \n=== SYNTHETIC ML model on REAL test set === \nROC-AUC:  0.851 \nAccuracy: 0.769 \nF1:       0.049 \nPrecision:0.025 \nRecall:   0.793 \nConfusion matrix:  \n[[2964  892]  \n[   6   23]]\n```\n\n## Real-\\>synthetic\n\nUse model built on real data to predict synthetic data:\n\n```         \n=== REAL ML model on SYNTHETIC test set === \nROC-AUC:  0.799 \nAccuracy: 0.500 \nF1:       0.000 \nPrecision:0.000 \nRecall:   0.000 \nConfusion matrix:  \n[[4000    0]  \n[4000    0]]\n```\n:::\n\n## 5. Result interpretation\n\n### Problem of model built on real data\n\n-   The real data features imbalanced numbers of patients and controls.\n\n-   The model trained on real data apparently had a higher AUC (0.856) than synthetic model (0.849).\n\n-   Real model identified every records as healthy controls (TP=0, FP=0).\n\n-   In use case 2, the real model behaved the same.\n\n-   I concluded: Without synthetic data, the model are biased by the imbalanced real data.\n\n## 5. Result interpretation\n\n**Advantage of model built on synthetic data**\n\n-   Synthetic data is balanced, meaning it has equal number of patients and healthy controls.\n\n-   On synthetic data, synthetic model gained reasonable metrics, including accuracy and AUC, meaning it is good at making correct decisions.\n\n-   In use case 1, synthetic model has low precision, meaning when it makes a positive classification, it is often wrong.\n\n-   However, this model may be used to rule out high risk people in clinical setting (if it says negative, it is often correct).\n\n## 5. Result interpretation\n\n**In summary, synthetic data is useful in improving disease risk prediction models.**\n\n# Thank You for Watching!\n\n",
    "supporting": [
      "presentation_files"
    ],
    "filters": [],
    "includes": {}
  }
}